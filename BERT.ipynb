{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlVjTCCdB0Oh",
        "colab_type": "code",
        "outputId": "98cea3cf-1806-41f6-980c-f11761603715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDZZmc39Mk1k",
        "colab_type": "code",
        "outputId": "1516ca1f-15ca-4360-bfc0-0f8723a52d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ce69bf715f2b0bfb8429a1736d20dbc9a2344d8b0ea5cd6f2602aee9c35915b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGYrwG7Lu5e",
        "colab_type": "code",
        "outputId": "111d5dfb-b057-402a-86a2-9293e080f812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "ma = torch.cuda.memory_allocated()\n",
        "print(ma, torch.cuda.max_memory_allocated(device=None))\n",
        "\n",
        "mc = torch.cuda.memory_cached()\n",
        "print(mc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "0 0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kudvK9i1MpZc",
        "colab_type": "code",
        "outputId": "9d70357b-0244-4ed4-c052-edaf9dc46490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "with open(\"train.json\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "instance = []\n",
        "instance_text = []\n",
        "instance_label = []\n",
        "for sub_data in data:\n",
        "    instance.append(sub_data)\n",
        "for item in instance:\n",
        "    instance_text.append(data[item][\"text\"])\n",
        "    instance_label.append(data[item][\"label\"])\n",
        "instance_text = np.array(instance_text)\n",
        "instance_label = np.array(instance_label)\n",
        "\n",
        "random.seed(10)\n",
        "with open(\"negative.json\") as json_file_neg:\n",
        "    data_neg = json.load(json_file_neg)\n",
        "instance_neg = []\n",
        "instance_text_neg = []\n",
        "instance_label_neg = []\n",
        "for sub_data in data_neg:\n",
        "    instance_neg.append(sub_data)\n",
        "for item in instance_neg:\n",
        "    instance_text_neg.append(data_neg[item][\"text\"])\n",
        "    instance_label_neg.append(0)\n",
        "instance_text_neg = np.array(instance_text_neg)\n",
        "instance_label_neg = np.array(instance_label_neg)\n",
        "all_instance_text = np.append(instance_text, instance_text_neg)\n",
        "all_instance_label = np.append(instance_label, instance_label_neg)\n",
        "print(len(instance_label), len(instance_label_neg))\n",
        "print(all_instance_label)\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in all_instance_text:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                    sent,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=128,\n",
        "                    pad_to_max_length=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_tensors='pt',\n",
        "               )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(all_instance_label)\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "ma = torch.cuda.memory_allocated()\n",
        "print(ma, torch.cuda.max_memory_allocated(device=None))\n",
        "\n",
        "mc = torch.cuda.memory_cached()\n",
        "print(mc)\n",
        "print(instance_text_neg[0])\n",
        "print(instance_text[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1168 1741\n",
            "[1 1 1 ... 0 0 0]\n",
            "2,618 training samples\n",
            "  291 validation samples\n",
            "2692884992 5964348928\n",
            "6585057280\n",
            "Plans for a third runway at Heathrow airport have been ruled illegal by the court of appeal because ministers did not adequately take into account the government’s commitments to tackle the climate crisis.The ruling is a major blow to the project at a time when public concern about the climate emergency is rising fast and the government has set a target in law of net zero emissions by 2050. The prime minister, Boris Johnson, could use the ruling to abandon the project, or the government could draw up a new policy document to approve the runway.The government is considering its next steps but will not appeal against the verdict. The transport secretary, Grant Shapps, said: “Our manifesto makes clear any Heathrow expansion will be industry-led. Airport expansion is core to boosting global connectivity and levelling up across the UK. We also take seriously our commitment to the environment.”Johnson has opposed the runway, saying in 2015 that he would “lie down in front of those bulldozers and stop the construction”. Heathrow is already one the busiest airports in the world, with 80 million passengers a year. The £14bn third runway could be built by 2028 and would bring 700 more planes per day and a big rise in carbon emissions.Johnson is thought to have been looking for a pretext to withdraw support for the extra runway and could make the argument for Birmingham to provide increased airport capacity for London given that train journey times will be reduced by HS2.The court’s ruling is the first major ruling in the world to be based on the Paris climate agreement and may have an impact both in the UK and around the globe by inspiring challenges against other high-carbon projects.Lord Justice Lindblom said: “The Paris agreement ought to have been taken into account by the secretary of state. The national planning statement was not produced as the law requires.” What just happened?For the first time judges have said that plans for a major infrastructure project are illegal because they breach the UK's commitments to reduce greenhouse gas emissions to tackle the climate crisis. This is a groundbreaking legal decision that could effect future infrastructure developments and puts the UK’s commitment to cut emission to net zero by 2050 at the forefront of future policymaking.What will happen next?The government has been told by the court of appeal to declare its decision to allow Heathrow airport expansion - contained in its airline national policy statement - illegal. Ministers have two choices now. They can withdraw the whole policy statement or try to amend it to make it compatible with the UK’s commitments to reduce greenhouse gas emissions to net zero by 2050. Will the runway be built?If the government can prove that expanding Heathrow is compatible with its commitments under the Paris agreement to very radically reduce greenhouse gas emissions, the runway may go ahead. But the prime minister has always been against the third runway, and the government has told the court it will not be appealing against its decision on Thursday. There now hangs a very big question mark over whether the bulldozers will ever start work on the runway.“It’s now clear that our governments can’t keep claiming commitment to the Paris agreement, while simultaneously taking actions that blatantly contradict it” said Tim Crosland, at legal charity Plan B, which brought the challenge. “The bell is tolling on the carbon economy loud and clear.”Plan B’s intervention was one of a number of legal challenges against the government’s national policy statement, which gave the go-ahead for the new runway in 2018 after MPs backed it by a large majority. Others were brought by local residents, councils, the mayor of London, and environmental groups including Friends of the Earth and Greenpeace.The challenges were dismissed in the high court in May 2019 but the complainants took their cases to the court of appeal, which delivered its verdicts on Thursday.Plan B argued that the Paris agreement target, which the government had ratified, was an essential part of government climate policy and that ministers had failed to assess how a third runway could be consistent with the Paris target of keeping global temperature rise as close to 1.5C as possible.“This is an opportunity for Boris Johnson to put Heathrow expansion to bed and focus on the most important diplomatic event of his premiership, the UN climate summit in Glasgow in November,” said Lord Randall, a former Conservative MP and climate adviser to the former prime minister Theresa May. “It’s his chance to shine on the world stage.” The court of appeal did not overturn the high court’s dismissal of the other challenges, which related to air and noise pollution, traffic, and the multibillion pound cost of the runway.But the Paris agreement ruling is far-reaching, according to Margaretha Wewerinke-Singh, an international public law expert at Leiden University, in the Netherlands. “Its implications are global,” she said.“For the first time, a court has confirmed that the Paris agreement temperature goal has binding effect. This goal was based on overwhelming evidence about the catastrophic risk of exceeding 1.5C of warming. Yet some have argued that the goal is aspirational only, leaving governments free to ignore it in practice.”Prof Corinne Le Quéré, at the University of East Anglia, said: “Government needs to put climate targets at the heart all big decisions, or risk missing their own net zero objectives with devastating consequences for climate and stability. I am relieved this is finally recognised in law.”Climate campaigner Greta Thunberg said: “Imagine when we all start taking the Paris agreement into account.”Heathrow and proponents of the third runway say it would provide an economic boost and is important for international business, particularly after Brexit. “The court of appeal dismissed all appeals against the government – including on ‘noise’ and ‘air quality’ – apart from one, [i.e. climate change] which is eminently fixable,” said a spokeswoman for Heathrow.“We will appeal [as an interested party] to the supreme court on this one issue and are confident that we will be successful. Expanding Heathrow, Britain’s biggest port and only hub, is essential to achieving the prime minister’s vision of global Britain. We will get it done the right way.”Mike Cherry, at the Federation of Small Businesses, said: “The verdict is a blow to small firms who need greater regional and global connectivity, as well as more opportunities to export.”However, most flights are taken for pleasure and just 20% of the UK population take more than two-thirds of international flights. Critics say the economic benefits are illusory given, for example, the estimated £10bn of taxpayers’ money needed to alter road and rail links to the airport, and would draw investment towards the south-east.“No amount of spin from Heathrow’s PR machine can obscure the carbon logic of a new runway,” said John Sauven, at Greenpeace UK. “Their plans would pollute as much as a small country.”Geraldine Nicholson, from local campaign group Stop Heathrow Expansion, said: “This is the final nail in the coffin for Heathrow expansion. We now need to make sure the threat of a third runway does not come back.”At a separate event on Thursday, Alok Sharma, the business secretary and president of November’s UN COP26 climate summit, said: “The only economy which can avoid the worst effects of climate change, and thus continue to deliver growth, is a decarbonised economy. Our choices will make or break the zero-carbon economy.”• This article was amended on February 28 2020. An earlier version had mistakenly called the business secretary Ashok Sharma, rather than Alok Sharma. This has been corrected. \n",
            "why houston flooding isn‘t a sign of climate change\n",
            "Distinguished US climate scientist, Dr Roy Spencer writes: \"In the context of climate change, is what we are seeing in Houston a new level of disaster which is becoming more common? The flood disaster unfolding in Houston is certainly very unusual. But so are other natural weather disasters, which have always occurred and always will occur....Major floods are difficult to compare throughout history because of the ways we alter the landscape., For example, as cities like Houston expand over the years, soil is covered up by roads, parking lots and buidings, with water rapidly draining off rather than soaking into the soil. The population of Houston is now ten times what is was in the 1920s.  The Houston metroplex has expanded greatly and the water drainage is basically in the direction of downtown Houston.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HLW0SCqNDp3",
        "colab_type": "code",
        "outputId": "626ba069-fa06-48dd-d537-eb6d72685d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "batch_size = 4\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "epochs = 4\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    655.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    655.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    655.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    655.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    655.    Elapsed: 0:00:16.\n",
            "  Batch   240  of    655.    Elapsed: 0:00:20.\n",
            "  Batch   280  of    655.    Elapsed: 0:00:23.\n",
            "  Batch   320  of    655.    Elapsed: 0:00:26.\n",
            "  Batch   360  of    655.    Elapsed: 0:00:29.\n",
            "  Batch   400  of    655.    Elapsed: 0:00:33.\n",
            "  Batch   440  of    655.    Elapsed: 0:00:36.\n",
            "  Batch   480  of    655.    Elapsed: 0:00:39.\n",
            "  Batch   520  of    655.    Elapsed: 0:00:42.\n",
            "  Batch   560  of    655.    Elapsed: 0:00:46.\n",
            "  Batch   600  of    655.    Elapsed: 0:00:49.\n",
            "  Batch   640  of    655.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.17\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    655.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    655.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    655.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    655.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    655.    Elapsed: 0:00:16.\n",
            "  Batch   240  of    655.    Elapsed: 0:00:19.\n",
            "  Batch   280  of    655.    Elapsed: 0:00:23.\n",
            "  Batch   320  of    655.    Elapsed: 0:00:26.\n",
            "  Batch   360  of    655.    Elapsed: 0:00:29.\n",
            "  Batch   400  of    655.    Elapsed: 0:00:32.\n",
            "  Batch   440  of    655.    Elapsed: 0:00:36.\n",
            "  Batch   480  of    655.    Elapsed: 0:00:39.\n",
            "  Batch   520  of    655.    Elapsed: 0:00:42.\n",
            "  Batch   560  of    655.    Elapsed: 0:00:45.\n",
            "  Batch   600  of    655.    Elapsed: 0:00:49.\n",
            "  Batch   640  of    655.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.20\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    655.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    655.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    655.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    655.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    655.    Elapsed: 0:00:16.\n",
            "  Batch   240  of    655.    Elapsed: 0:00:20.\n",
            "  Batch   280  of    655.    Elapsed: 0:00:23.\n",
            "  Batch   320  of    655.    Elapsed: 0:00:26.\n",
            "  Batch   360  of    655.    Elapsed: 0:00:29.\n",
            "  Batch   400  of    655.    Elapsed: 0:00:33.\n",
            "  Batch   440  of    655.    Elapsed: 0:00:36.\n",
            "  Batch   480  of    655.    Elapsed: 0:00:39.\n",
            "  Batch   520  of    655.    Elapsed: 0:00:42.\n",
            "  Batch   560  of    655.    Elapsed: 0:00:46.\n",
            "  Batch   600  of    655.    Elapsed: 0:00:49.\n",
            "  Batch   640  of    655.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    655.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    655.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    655.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    655.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    655.    Elapsed: 0:00:16.\n",
            "  Batch   240  of    655.    Elapsed: 0:00:19.\n",
            "  Batch   280  of    655.    Elapsed: 0:00:23.\n",
            "  Batch   320  of    655.    Elapsed: 0:00:26.\n",
            "  Batch   360  of    655.    Elapsed: 0:00:29.\n",
            "  Batch   400  of    655.    Elapsed: 0:00:32.\n",
            "  Batch   440  of    655.    Elapsed: 0:00:36.\n",
            "  Batch   480  of    655.    Elapsed: 0:00:39.\n",
            "  Batch   520  of    655.    Elapsed: 0:00:42.\n",
            "  Batch   560  of    655.    Elapsed: 0:00:45.\n",
            "  Batch   600  of    655.    Elapsed: 0:00:49.\n",
            "  Batch   640  of    655.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.22\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:03:38 (h:mm:ss)\n",
            "Predicting labels for 2,909 test sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIO9vTrJmwSl",
        "colab_type": "code",
        "outputId": "50f7333b-3337-4878-8f85-41de52eba1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "with open(\"test-unlablled.json\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "instance = []\n",
        "instance_text = []\n",
        "instance_label = []\n",
        "for sub_data in data:\n",
        "    instance.append(sub_data)\n",
        "for item in instance:\n",
        "    instance_text.append(data[item][\"text\"])\n",
        "instance_text = np.array(instance_text)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(instance_text.shape[0]))\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in instance_text:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sent,  # Sentence to encode.\n",
        "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "        max_length=128,  # Pad & truncate all sentences.\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,  # Construct attn. masks.\n",
        "        return_tensors='pt',  # Return pytorch tensors.\n",
        "    )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 4\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions = []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "climate_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "final_data = pd.DataFrame(climate_predictions)\n",
        "final_data.to_csv(\"predict_data.csv\")\n",
        "print(climate_predictions)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 100\n",
            "\n",
            "[1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1]\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}