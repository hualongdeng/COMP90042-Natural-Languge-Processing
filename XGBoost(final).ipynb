{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5hC7MlZfzTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction import text\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(text.ENGLISH_STOP_WORDS)\n",
        "rex = re.compile(r'[!\"#$%&\\()*+,./:;<=>?@\\\\^_{|}~]+')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_review(raw_review: str) -> str:\n",
        "    review_text = rex.sub(' ', raw_review)\n",
        "    review_text = review_text.lower()\n",
        "    word_list = review_text.split()\n",
        "    tokens = list(map(lemmatizer.lemmatize, word_list))\n",
        "    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n",
        "    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n",
        "    return meaningful_words\n",
        "\n",
        "def list2Str(stringList, length):\n",
        "    str_result = []\n",
        "    for item in stringList:\n",
        "        result = ''\n",
        "        for word in item[:length]:\n",
        "            result = result + ' ' + word\n",
        "        str_result.append(result)\n",
        "    return str_result\n",
        "\n",
        "def getTrainData(file, label):\n",
        "    with open(file) as json_file:\n",
        "      data = json.load(json_file)\n",
        "      instance = []\n",
        "      instance_text = []\n",
        "      instance_label = []\n",
        "      for sub_data in data:\n",
        "          instance.append(sub_data)\n",
        "      for item in instance:\n",
        "          instance_text.append(clean_review(data[item][\"text\"]))\n",
        "          instance_label.append(label)\n",
        "      instance_text = list2Str(instance_text, length=512)\n",
        "      instance_text = np.array(instance_text)\n",
        "      instance_label = np.array(instance_label)\n",
        "      return instance_text, instance_label\n",
        "\n",
        "def getTestData(file):\n",
        "    with open(file) as json_file:\n",
        "      data = json.load(json_file)\n",
        "      instance = []\n",
        "      instance_text = []\n",
        "      for sub_data in data:\n",
        "          instance.append(sub_data)\n",
        "      for item in instance:\n",
        "          instance_text.append(clean_review(data[item][\"text\"]))\n",
        "      instance_text = list2Str(instance_text, length=512)\n",
        "      instance_text = np.array(instance_text)\n",
        "      return instance_text\n",
        "\n",
        "pos_text, pos_label = getTrainData(\"train.json\", 1)\n",
        "cli_text, cli_label = getTrainData(\"climate.json\", 0)\n",
        "non_cli_text, non_cli_label = getTrainData(\"no_climate.json\", 0)\n",
        "dev_text = getTestData(\"test-unlabelled.json\")\n",
        "print(len(pos_label), len(cli_label), len(non_cli_label), len(dev_text))\n",
        "X = np.append(pos_text, cli_text, axis=0)\n",
        "X = np.append(X, non_cli_text, axis = 0)\n",
        "X = np.append(X, dev_text, axis = 0)\n",
        "Y = np.append(pos_label, cli_label)\n",
        "Y = np.append(Y, non_cli_label)\n",
        "dev_len = len(dev_text)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv=TfidfVectorizer(binary=False,decode_error='ignore',stop_words='english')\n",
        "vec=cv.fit_transform(X)\n",
        "new_X=vec.toarray()\n",
        "\n",
        "pos_X = new_X[:len(pos_label)]\n",
        "cli_X = new_X[len(pos_label):(len(pos_label) + len(cli_label))]\n",
        "non_cli_X = new_X[(len(pos_label) + len(cli_label)):(len(pos_label) + len(cli_label) + len(non_cli_label))]\n",
        "dev_X = new_X[-dev_len:]\n",
        "print(new_X.shape)\n",
        "print(pos_X.shape)\n",
        "print(cli_X.shape)\n",
        "print(non_cli_X.shape)\n",
        "print(dev_X.shape)\n",
        "\n",
        "X_train = np.append(pos_X, non_cli_X, axis=0)\n",
        "Y_train = np.append(pos_label, non_cli_label)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMtjEKSVf8dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
        "ddev = xgb.DMatrix(dev_X)  \n",
        "param = {'max_depth':6, 'eta':0.05, 'eval_metric':'merror', 'silent':0, 'objective':'multi:softmax', 'num_class':2}\n",
        "evallist  = [(dtrain,'train')]\n",
        "num_round = 200\n",
        "bst = xgb.train(param, dtrain, num_round, evallist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_2phUR9gHYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_cli = bst.predict(ddev)\n",
        "print(preds_cli)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSml-BobgIE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.append(pos_X, cli_X, axis=0)\n",
        "Y_train = np.append(pos_label, cli_label)\n",
        "dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
        "param = {'max_depth':6, 'eta':0.05, 'eval_metric':'merror', 'silent':0, 'objective':'multi:softmax', 'num_class':2}\n",
        "evallist  = [(dtrain,'train')]\n",
        "num_round = 200\n",
        "bst1 = xgb.train(param, dtrain, num_round, evallist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPPde43SgR_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_mis = bst1.predict(ddev)\n",
        "print(preds_mis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0k46THjgU4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(preds_mis)):\n",
        "    if preds_cli[i] == 0:\n",
        "        preds_mis[i] = 0\n",
        "final_data = pd.DataFrame(preds_mis)\n",
        "final_data.to_csv(\"predict_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJl2Ni_HgiOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_positive = pd.read_csv(\"predict_data.csv\")\n",
        "positive_len = len(raw_positive)\n",
        "positive_data = []\n",
        "for i in range(0, positive_len):\n",
        "    row = np.array(raw_positive.iloc[i])\n",
        "    positive_data.append(row[1])\n",
        "positive_data = np.array(positive_data)\n",
        "print(positive_data)\n",
        "\n",
        "dict = {}\n",
        "submit = './test-output.json'\n",
        "i = 0\n",
        "print(len(np.argwhere(positive_data == 1)))\n",
        "for item in positive_data:\n",
        "    temp_dict = {}\n",
        "    temp_dict['label'] = int(item)\n",
        "    print(temp_dict)\n",
        "    dict[\"test-\" + str(i)] = temp_dict\n",
        "    i = i + 1\n",
        "print(dict)\n",
        "with open(submit, 'w') as f:\n",
        "    json.dump(dict, f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}